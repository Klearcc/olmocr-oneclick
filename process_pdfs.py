import os
import datetime
import torch
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration
from olmocr.data.renderpdf import render_pdf_to_base64png
from olmocr.prompts import build_finetuning_prompt, anchor as anchor_utils
from PyPDF2 import PdfReader
from PIL import Image
from io import BytesIO
import base64
import logging

# é…ç½®æ—¥å¿—ç³»ç»Ÿï¼š
logging.basicConfig(filename='processing_log.log', level=logging.INFO,format='%(asctime)s %(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

# å•ä¾‹æ¨¡å¼ç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸï¼š
class ModelManager:
    _instance = None

    @staticmethod
    def get_model():
        if ModelManager._instance is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = Qwen2VLForConditionalGeneration.from_pretrained(
                "allenai/olmOCR-7B-0225-preview",
                torch_dtype=torch.bfloat16).to(device).eval()
            
            processor = AutoProcessor.from_pretrained(
                "Qwen/Qwen2-VL-7B-Instruct", use_fast=True)

            logger.info(f"Model loaded successfully to {device}")
            
            ModelManager._instance = (model, processor, device)
        return ModelManager._instance


def process_pdf_batch(pdf_path, page_numbers, temperature=0.4,max_new_tokens=4096):
    model, processor, device = ModelManager.get_model()
    
    batch_texts,batch_images=[],[]
    
    for page_number in page_numbers:
        try:
            print(f"ğŸ” å¼€å§‹å¤„ç†ç¬¬ {page_number} é¡µ")
            
            image_base64=render_pdf_to_base64png(pdf_path,page_number,target_longest_image_dim=1024)
            print(f"âœ… ç¬¬ {page_number} é¡µå›¾ç‰‡æ¸²æŸ“å®Œæˆ")
            
            anchor_text=anchor_utils.get_anchor_text(pdf_path,page_number,pdf_engine="pdfreport",target_length=4000)
            print(f"âœ… ç¬¬ {page_number} é¡µ Anchor æ–‡æœ¬æŠ½å–å®Œæˆ")

            prompt=build_finetuning_prompt(anchor_text)
            
            messages=[
                {
                    "role":"user",
                    "content":[
                        {"type":"text","text":prompt},
                        {"type":"image_url","image_url":{"url":f"data:image/png;base64,{image_base64}"}}
                    ]
                }
            ]

            text_input=processor.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)
            
            image_obj_pil = Image.open(BytesIO(base64.b64decode(image_base64)))
        
        except Exception as e:
            error_detail=f"[Page {page_number}] Preparation failed:{e}"
            
            logger.warning(error_detail) 
            
            # æ·»åŠ æ­¤è¡Œä»¥ä¾¿å³æ—¶æŸ¥çœ‹é”™è¯¯ï¼š
            print(error_detail)

            text_input=""
          
           # ä¸ºé¿å…é”™è¯¯ä¸­æ–­åç»­æ¨ç†ï¼Œè¿™é‡Œåˆ›å»ºç®€å•ç©ºç™½å›¾åƒã€‚
           # æ³¨æ„å®é™…åœºæ™¯ä¸­å¯èƒ½éœ€è·³è¿‡ï¼Œè§†éœ€æ±‚è€Œå®šï¼
            image_obj_pil = Image.new('RGB',(224,224),'white')

        batch_texts.append(text_input)
        batch_images.append(image_obj_pil)

    try:
        inputs_dict_tensorized = processor(
            text=batch_texts,
            images=batch_images,
            padding=True,
            return_tensors="pt"
        ).to(device)

        with torch.no_grad():
            outputs=model.generate(**inputs_dict_tensorized,max_new_tokens=max_new_tokens,temperature=temperature)

        print("ğŸš€ æ¨¡å‹ç”Ÿæˆç»“æŸ") 

    except Exception as inference_err:
        err_msg=f"[Inference Error]:{inference_err}"
         
        logger.error(err_msg)  
         
        # å®æ—¶æ˜¾ç¤ºï¼š
        print(err_msg)
         
        return["âš ï¸ æ¨ç†è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸ï¼"]*len(page_numbers)


    prompt_lengths=[len(processor.tokenizer(txt)["input_ids"])if txt else 0 for txt in batch_texts]

    decoded_results=[]
    
    for idx,out_seq in enumerate(outputs):
        prompt_len=min(prompt_lengths[idx],len(out_seq))
       
        new_token_ids_for_output_slice_based_on_input_length_calc_above = out_seq[prompt_len:]
       
        single_decoded_output_per_page_final = processor.tokenizer.decode(
            new_token_ids_for_output_slice_based_on_input_length_calc_above,
            skip_special_tokens=True).strip()

        if not single_decoded_output_per_page_final.strip():
            single_decoded_output_per_page_final="âš ï¸ æœ¬é¡µå†…å®¹è§£æä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥"

        decoded_results.append(single_decoded_output_per_page_final)

    return decoded_results


def process_entire_pdf(pdf_path, batch_size: int = 1): 
    original_file_name = os.path.splitext(os.path.basename(pdf_path))[0]
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")

    output_dir = "/app/output_results"
    os.makedirs(output_dir, exist_ok=True)  # è‡ªåŠ¨åˆ›å»ºç›®æ ‡ç›®å½•

    result_txt_path=os.path.join(
        output_dir,
        f"{original_file_name}_{timestamp}.txt"
    )


    pdf_reader_instance_properly_named_concisely = PdfReader(pdf_path)       
    num_pages = len(pdf_reader_instance_properly_named_concisely.pages)

    header = f"ğŸ“˜ PDFæ–‡ä»¶åç§°ï¼š{original_file_name}.pdfï¼Œå…± {num_pages} é¡µ\n"

    # ç›´æ¥æ‰“å¼€æ–‡ä»¶å‡†å¤‡å®æ—¶é€é¡µä¿å­˜
    with open(result_txt_path, "w", encoding="utf-8") as out_f_handle:
        
        logger.info(header)
        out_f_handle.write(header)
        
        pages_range_all = list(range(1, num_pages + 1))

        for start_idx in range(0, len(pages_range_all), batch_size):
            end_idx = min(start_idx + batch_size, len(pages_range_all))
            current_group_of_pages_processing_in_loop = pages_range_all[start_idx:end_idx]

            logger.info(f"[Processing Batch] Pages:{current_group_of_pages_processing_in_loop}")

            try:
                batched_outputs_gpu_call_efficient_once_only=process_pdf_batch(
                    pdf_path,
                    current_group_of_pages_processing_in_loop
                )

                for relative_index, page_num in enumerate(current_group_of_pages_processing_in_loop):
                    formatted_page_result_str = (
                        f"\n==== ç¬¬ {page_num}/{num_pages} é¡µåˆ†æç»“æœ ====\n"
                        f"{batched_outputs_gpu_call_efficient_once_only[relative_index]}\n")

                    # å®æ—¶é€é¡µï¼ˆæˆ–é€æ‰¹ï¼‰å†™å…¥åˆ°æ–‡ä»¶è€Œéä¸´æ—¶ç¼“å­˜ï¼
                    out_f_handle.write(formatted_page_result_str)

                    # æ–°å¢ï¼šæ§åˆ¶å°å®æ—¶æ˜¾ç¤ºæ¯é¡µå¤„ç†æƒ…å†µ
                    print(f"âœ… å·²å®Œæˆæ–‡ä»¶ã€Œ{original_file_name}.pdfã€ç¬¬ {page_num}/{num_pages} é¡µçš„å¤„ç†")

                # å†™å®Œæ¯ä¸ªbatchä¹‹åä¸»åŠ¨æ¸…ç†å†…å­˜å’ŒGPUç¼“å­˜ï¼š
                torch.cuda.empty_cache()
                import gc; gc.collect()

            except Exception as err:
                error_detail=f"[Pages {current_group_of_pages_processing_in_loop}] Error: {str(err)}"
                
                logger.error(error_detail)
                
                for failed_page in current_group_of_pages_processing_in_loop:
                    formatted_error_str=(
                        f"\n==== ç¬¬ {failed_page}/{num_pages} é¡µåˆ†æç»“æœ ====\nâš ï¸ æœ¬é¡µå†…å®¹è§£æé”™è¯¯ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥ï¼\n"
                    )
                    
                    out_f_handle.write(formatted_error_str)

    completion_message=f"\nâœ… Done! åˆ†æå®Œæ¯•å·²ä¿å­˜ä¸º: ã€Œ{result_txt_path}ã€\n"

    print(completion_message)
    logger.info(completion_message)            


# æ–°å¢å‡½æ•°ï¼šè‡ªåŠ¨éå†æŒ‡å®šç›®å½•åŠå…¶ä¸‹æ‰€æœ‰å­ç›®å½•ï¼Œå¹¶è¿”å›å…¨éƒ¨pdfæ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
def find_all_pdfs(root_dir='./pdf'):
    pdf_files_list = []

    for root, directories, filenames in os.walk(root_dir):
        for filename in filenames:
            if filename.lower().endswith(".pdf"):
                fullpath = os.path.join(root, filename)
                pdf_files_list.append(fullpath)

    return sorted(pdf_files_list)



if __name__ == "__main__":
    try:
        all_pdf_paths = find_all_pdfs('./pdf')

        print(f"ä¸€å…±æ‰¾åˆ° {len(all_pdf_paths)} ä¸ª PDF å¾…å¤„ç†ï¼Œå¼€å§‹æ‰¹é‡æ‰§è¡Œ...")

        for pdf_path in all_pdf_paths:
            process_entire_pdf(pdf_path, batch_size=1)  # å¯è°ƒæ•´batchå¤§å°

    except Exception as main_err:
        error_main_run = f"[Main Error] ç¨‹åºè¿è¡Œæ—¶å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥è·¯å¾„æˆ–ç¯å¢ƒé…ç½®æ˜¯å¦æ­£ç¡®! é”™è¯¯è¯¦æƒ…: {main_err}"
        
        print(error_main_run)


